import os
import json
import subprocess
from datetime import datetime
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from ultralytics import YOLO


# ğŸ”¹ exiftoolì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
def get_metadata(image_file):
    result = subprocess.run(
        ["exiftool", image_file],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        universal_newlines=True,
    )

    metadata = {
        "filename": os.path.basename(image_file),
        "description": "",
        "location": "",
        "datetime": "",
        "uuid": os.path.splitext(os.path.basename(image_file))[0],
    }

    for line in result.stdout.splitlines():
        if "Date/Time Original" in line:
            metadata["datetime"] = line.split(":", 1)[1].strip()
        elif "GPS Position" in line:
            metadata["location"] = line.split(":", 1)[1].strip()
        elif "Image Description" in line:
            metadata["description"] = line.split(":", 1)[1].strip()

    if not metadata["datetime"]:
        metadata["datetime"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    return metadata


# ğŸ”¹ BLIP ì„¤ëª… ìƒì„±
def generate_caption(image_path, processor, model):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(image, return_tensors="pt").to("cpu")
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)


# ğŸ”¹ YOLO ê°ì²´ íƒì§€ ê²°ê³¼ ì¶”ì¶œ
def detect_objects(image_path, model):
    results = model(image_path)
    labels = results[0].boxes.cls.cpu().tolist()
    names = results[0].names
    return list(set([names[int(idx)] for idx in labels]))


# ğŸ”¹ ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
def load_yolo_model():
    return YOLO("yolov8n.pt")


def load_blip_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model


# ğŸ”¹ í†µí•© ì „ì²˜ë¦¬ í•¨ìˆ˜
def process_image(image_file, processor, blip_model, yolo_model):
    metadata = get_metadata(image_file)
    metadata["file_name"] = os.path.basename(image_file)  # âœ… ìš” ì¤„ì„ ì¶”ê°€

    caption = generate_caption(image_file, processor, blip_model)
    labels = detect_objects(image_file, yolo_model)

    text = f"ì´ ì´ë¯¸ì§€ì—ëŠ” {', '.join(labels)} ë“±ì´ ìˆìœ¼ë©°, ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {caption}"

    return {
        "meta": metadata,
        "text": text
    }